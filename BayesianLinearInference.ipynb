{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import diff\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy.linalg import eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data.dat') # load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.errorbar(data[:,0], data[:,1], data[:,2], fmt='o')\n",
    "plt.xlabel(\"x\", fontsize=20)\n",
    "plt.ylabel(\"D(x)\", fontsize=20)\n",
    "plt.xlim([0,1.6])\n",
    "#plt.legend()\n",
    "plt.title('Data plot with error bars', fontdict=None, loc='center', pad=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:,0]\n",
    "y_noise = np.log(data[:,1]) # the inference is done on log of the data\n",
    "y_err = (data[:,2])/data[:,1] # error on log(data)\n",
    "beta_err = (1/y_err)/np.max(1/y_err) # the known error is trated as a weight on y_noise data\n",
    "N_tot = x.size # total number of input data \n",
    "y_noise=y_noise*beta_err # the data are weighted for the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation of LogEvidence for different models (labeled by M_v), in the present case the model is $\\sum_{j=0}^M_v w_j*x^(2j)$ (monomia basis function), w_j are the parameters to be inferred\n",
    "# The definition of the LogEvidence function is in Ref. ....\n",
    "M_max=21 # Maximum value of M_v for which the LogEvidence function is computed \n",
    "log_evidence_vP= np.zeros((M_max))\n",
    "alpha_vP = np.zeros((M_max)) #initializing \\alpha\n",
    "beta_vP = np.zeros((M_max)) #initializing \\beta\n",
    "g_vP = np.zeros((M_max))\n",
    "# the LogEvidence function is computed in a semianalytical way (see ...). The integration on the ... is performed analytically.\n",
    "# In the following the numerical solution for optimal value of \\alpha and \\beta is found. \n",
    "alpha0 = 0.001 #0.001\n",
    "beta0 = 240 #240\n",
    "for M_v in range(2,M_max): # M_v: maximum degree of monomia for each models, i.e. number of parameters\n",
    "    Phi_vP= np.zeros((N, M_v))\n",
    "    for j in range(0,M_v):\n",
    "        Phi_vP[:,j]=x**(2*j)*beta_err # monomia basis functions\n",
    "    li_vP0,ei_vP=eig(np.dot(Phi_vP.T,Phi_vP))\n",
    "    delta_alphaP = 1\n",
    "    delta_betaP = 1 \n",
    "    alphaP = alpha0\n",
    "    betaP = beta0\n",
    "    conta = 0\n",
    "    while abs(delta_alphaP) > 1e-3 or abs(delta_betaP) > 1 and conta < 1e4:\n",
    "        li_vP=li_vP0*betaP\n",
    "        SN_vP = np.linalg.inv(alphaP*np.identity(M_v)+betaP*np.dot(Phi_vP.T,Phi_vP))\n",
    "        mN_vP=betaP*np.dot(np.dot(SN_vP,Phi_vP.T),y_noise) \n",
    "        g_vP = np.sum(li_vP.real/(alphaP+li_vP.real)) \n",
    "        alpha1P = g_vP/(np.dot(mN_vP.T,mN_vP)) \n",
    "        beta1P = 1/(1/(N-g_vP)*np.sum((y_noise-np.dot(Phi_vP,mN_vP.T))**2))\n",
    "        delta_alphaP = alpha1P-alphaP\n",
    "        alphaP = alpha1P \n",
    "        delta_betaP = beta1P-betaP\n",
    "        betaP = beta1P\n",
    "        conta=conta+1\n",
    "    del conta\n",
    "    alpha_vP[M_v]=alphaP\n",
    "    beta_vP[M_v]=betaP\n",
    "    A_vP=np.linalg.inv(SN_vP) \n",
    "    E_mNs_vP=betaP/2*(y_noise-np.dot(Phi_vP,mN_vP.T))**2 \n",
    "    E_mN_vP=E_mNs_vP.sum()\n",
    "    log_evidence_vP[M_v]=M_v/2*np.log(alpha_vP[M_v])+N/2*np.log(beta_vP[M_v])-E_mN_vP-1/2*np.log(np.linalg.det(A_vP))-N/2*np.log(2*math.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(log_evidence_vP,'-*',label='monomia basis')\n",
    "plt.xlabel(\"Number of parameters\", fontsize=15)\n",
    "plt.ylabel(\"Log-Evidence\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_optP= np.where(log_evidence_vP==max(log_evidence_vP[3:]))[0]\n",
    "M_optP= M_optP.item()\n",
    "print('Monomia basis: Optimal number of parameters',M_optP,' - Maximum of the Log-Evidence function',log_evidence_vP[M_optP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the predictive ... (see [])\n",
    "x_infer = np.linspace(0, 1.6, 50)\n",
    "# averaging predictive estimation of different models [] (labeled by number of parameters M) in the interval [M_max-L1,M_max+L2], where M_max: max(Evidence)=Evidence(M_max)\n",
    "# L1 = L2 = 0 --> no averge\n",
    "L1=0 \n",
    "L2=0 \n",
    "meanN_optPj=np.zeros([L1+L2+1,x_infer.size])\n",
    "sigmaN_optPj=np.zeros([L1+L2+1,x_infer.size]) \n",
    "sigma_meanN_optPj=np.zeros([L1+L2+1,x_infer.size])\n",
    "#\n",
    "for j_optP in range (M_optP-L1,M_optP+L2+1):\n",
    "    Phi_inferoptP = np.zeros((x_infer.size, j_optP))\n",
    "    Phi_optP = np.zeros((x.size, j_optP))\n",
    "    alpha_optP = alpha_vP[j_optP]\n",
    "    beta_optP = beta_vP[j_optP] \n",
    "    for j in range(0,j_optP):\n",
    "        Phi_inferoptP[:,j]=x_infer**(2*j)\n",
    "    for j in range(0,j_optP):\n",
    "        Phi_optP[:,j]=x**(2*j)*beta_err\n",
    "    A_optP=alpha_optP*np.identity(j_optP)+beta_optP*np.dot(Phi_optP.T,Phi_optP)\n",
    "    SN_optP=np.linalg.inv(A_optP)\n",
    "    sigmaN_optP=1/beta_optP+np.dot(Phi_inferoptP,np.dot(SN_optP,Phi_inferoptP.T))\n",
    "    sigma_meanN_optP=np.diag(sigmaN_optP)**0.5\n",
    "    mN_optP=beta_optP*np.dot(np.dot(SN_optP,Phi_optP.T),y_noise)\n",
    "    meanN_optP=np.dot(Phi_inferoptP,mN_optP.T)\n",
    "    meanN_optPj[j_optP-M_optP+L1-L2,:]=meanN_optP*(log_evidence_vP[j_optP])\n",
    "    sigmaN_optPj[j_optP-M_optP+L1-L2,:]=meanN_optP**2*(log_evidence_vP[j_optP])\n",
    "    sigma_meanN_optPj[j_optP-M_optP+L1-L2,:]=sigma_meanN_optP\n",
    "meanN_optPtot=np.sum(meanN_optPj,axis=0)/np.sum((log_evidence_vP[M_optP-L1:M_optP+L2+1])) # predictive estimation \n",
    "sigmaN_optPtot=np.sum(sigmaN_optPj,axis=0)/np.sum((log_evidence_vP[M_optP-L1:M_optP+L2+1]))-meanN_optPtot**2 # error on the predictive estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.errorbar(x_infer,np.exp(meanN_optPtot),np.sqrt(np.abs(sigmaN_optPtot))+np.sum(sigma_meanN_optPj,axis = 0),label='predictive optimal number of paramaters - Monomia basis')\n",
    "plt.errorbar(data[:,0],data[:,1],data[:,2], fmt='o',label='data')\n",
    "plt.ylim([-0.05, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the LogEvidence function as a function of the number of input data taken progressively under account\n",
    "log_evidence_vP = np.zeros((M_max))\n",
    "alpha_vP = np.zeros((M_max))\n",
    "beta_vP = np.zeros((M_max))\n",
    "g_vP = np.zeros((M_max))\n",
    "M_optP = np.zeros((N_tot))\n",
    "log_evidenceP = np.zeros((N_tot))\n",
    "alphaP_Nv = np.zeros((N_tot))\n",
    "betaP_Nv = np.zeros((N_tot))\n",
    "N_min=10 # minimum number of input data for which the LogEvidence is computed\n",
    "for N_v in range(N_min,N_tot):\n",
    "    y_noise = np.log(data[:N_v,1])\n",
    "    x = data[:N_v,0]\n",
    "    y_err = data[:N_v,2]/data[:N_v,1]\n",
    "    beta_err = (1/y_err)/np.max(1/y_err)\n",
    "    y_noise = y_noise*beta_err \n",
    "    alpha0 = 0.001 # initializing \\alpha\n",
    "    beta0 = 240 # initializing \\beta\n",
    "    for M_v in range(2,M_tot): # number of parameters\n",
    "        Phi_vP= np.zeros((N_v, M_v))\n",
    "        for j in range(0,M_v):\n",
    "            Phi_vP[:,j]=x**(2*j)*beta_err\n",
    "        li0_vP,ei_vP=eig(np.dot(Phi_vP.T,Phi_vP))\n",
    "        delta_alphaP = 1\n",
    "        delta_betaP = 1 \n",
    "        alphaP = alpha0\n",
    "        betaP = beta0\n",
    "        conta = 0 \n",
    "        while abs(delta_alphaP) > 1e-3 or abs(delta_betaP) > 1 and conta < 1e4:\n",
    "            li_vP=li0_vP*betaP\n",
    "            SN_vP = np.linalg.inv(alphaP*np.identity(M_v)+betaP*np.dot(Phi_vP.T,Phi_vP))\n",
    "            mN_vP=betaP*np.dot(np.dot(SN_vP,Phi_vP.T),y_noise) \n",
    "            g_vP = np.sum(li_vP.real/(alphaP+li_vP.real)) \n",
    "            alpha1P = g_vP/(np.dot(mN_vP.T,mN_vP)) \n",
    "            beta1P = 1/(1/(N_v-g_vP)*np.sum((y_noise-np.dot(Phi_vP,mN_vP.T))**2))\n",
    "            delta_alphaP = alpha1P-alphaP\n",
    "            alphaP = alpha1P \n",
    "            delta_betaP = beta1P-betaP\n",
    "            betaP = beta1P\n",
    "            conta = conta +1\n",
    "        alpha_vP[M_v]=alphaP\n",
    "        beta_vP[M_v]=betaP\n",
    "        A_vP=np.linalg.inv(SN_vP) \n",
    "        E_mNs_vP=betaP/2*(y_noise-np.dot(Phi_vP,mN_vP.T))**2 \n",
    "        E_mN_vP=E_mNs_vP.sum()\n",
    "        log_evidence_vP[M_v]=M_v/2*np.log(alpha_vP[M_v])+N_v/2*np.log(beta_vP[M_v])-E_mN_vP-1/2*np.log(np.linalg.det(A_vP))-N_v/2*np.log(2*math.pi)\n",
    "    M_optPv = np.where(log_evidence_vP==max(log_evidence_vP))[0] # optimal number of parameters (estimated by the maximum of the LogEvidence function for each number of input data)\n",
    "    M_optP[N_v] = M_optPv.item()\n",
    "    log_evidenceP[N_v] = log_evidence_vP[M_optPv] # LogEvidence as a function of the number of input data progressively taken under account for the optimal number of parameters (for which the LogEvidence function is maximum at a given number of input data)\n",
    "    alphaP_Nv[N_v] = alpha_vP[M_optPv] # optimal alpha as a function of the number of input data progressively taken under account\n",
    "    betaP_Nv[N_v] = beta_vP[M_optPv] # optimal beta as a function of the number of input data progressively taken under consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the LogEvidence function/number of input data as a function of the number of input data \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(log_evidenceP/np.linspace(1, N_tot, N_tot),'-*',label='Monomia basis')\n",
    "plt.xlabel(\"Number of input data:N_i\", fontsize=15)\n",
    "plt.ylabel(\"Log-Evidence/N_i\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the optimal number of parameters as a function of the number of input data\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(M_optP,'-*',label='Monomia basis')\n",
    "plt.xlabel(\"Number of input data:N_i\", fontsize=15)\n",
    "plt.ylabel(\"Optimal number of parameters\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the value at x=0 from the predictive ... as a function of the number of input data taken under account\n",
    "x_infer0 = np.linspace(0, 1, 2)\n",
    "datax0 = np.zeros((N_tot))\n",
    "err_datax0 = np.zeros((N_tot))\n",
    "for N_v in range(10,N_tot):\n",
    "    y_noise = np.log(data[:N_v,1])\n",
    "    y_err = np.abs(np.log(data[:N_v,2]))\n",
    "    beta_err = (1/y_err)\n",
    "    y_noise = y_noise*beta_err\n",
    "    x = data[:N_v,0]\n",
    "    M_optP0 = np.int(M_optP[N_v])\n",
    "    Phi_inferoptP0 = np.zeros((x_infer0.size, M_optP0))\n",
    "    Phi_optP0 = np.zeros((x.size, M_optP0))\n",
    "    alpha_optP0 = alphaP_Nv[N_v]\n",
    "    beta_optP0 = betaP_Nv[N_v] \n",
    "    for j in range(0,M_optP0):\n",
    "        Phi_inferoptP0[:,j] = x_infer0**(2*j)\n",
    "    for j in range(0,M_optP0):\n",
    "        Phi_optP0[:,j] = x**(2*j)*beta_err\n",
    "    A_optP0 = alpha_optP0*np.identity(M_optP0)+beta_optP0*np.dot(Phi_optP0.T,Phi_optP0)\n",
    "    SN_optP0 = np.linalg.inv(A_optP0)\n",
    "    sigmaN_optP0 = 1/beta_optP0+np.dot(Phi_inferoptP0,np.dot(SN_optP0,Phi_inferoptP0.T))\n",
    "    sigma_meanN_optP0 = np.diag(sigmaN_optP0)**0.5\n",
    "    mN_optP0 = beta_optP0*np.dot(np.dot(SN_optP0,Phi_optP0.T),y_noise)\n",
    "    meanN_optP0 = np.dot(Phi_inferoptP0,mN_optP0.T)\n",
    "    datax0[N_v] = np.exp(meanN_optP0[0]) # predictive value of data at x=0 as a function of the number of parameters progressively taken under account\n",
    "    err_datax0[N_v] = sigma_meanN_optP0[0] # error on the predictive value of data at x=0 as a function of the number of parameters progressively taken under account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predictive value of data at x=0 as a function of the number of input data\n",
    "xplot = data[:,0]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.errorbar(xplot,datax0,err_datax0,fmt='*',label='polynomial-basis')\n",
    "plt.ylabel(\"data\", fontsize=20)\n",
    "plt.xlabel(\"x_max\", fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
